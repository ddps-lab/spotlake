# Azure SpotLake Collector (Batch Test Environment)

[English](README.md) | [Korean](README_kr.md)

This directory contains the **Test Environment** implementation of the Azure Spot Instance data collector. It mirrors the structure of the production `azure/batch` directory but is configured to operate in isolation, writing data to test resources while reading configuration from production resources where necessary.

## Key Differences from Production

| Feature | Production (`azure/batch`) | Test (`azure/batch-test`) |
| :--- | :--- | :--- |
| **S3 Write Bucket** | `spotlake` | `spotlake-test` |
| **S3 Read Bucket** | `spotlake` | `spotlake` (Config/Metadata), `spotlake-test` (Raw Data/State) |
| **DynamoDB Table** | `azure` | `azure-test` |
| **Infrastructure** | `spotlake-azure-compute-env` | `spotlake-azure-test-compute-env` |
| **Monitoring** | Basic Logs | **Enhanced**: Memory Usage & Execution Time Logging |

## Directory Structure

```graphql
azure/batch-test/
├── if/                 # Ported IF collector (Writes to spotlake-test)
├── price/              # Ported Price collector (Writes to spotlake-test)
├── sps/                # Ported SPS collector (Reads Prod Config, Writes Test Data)
├── merge/              # Ported Merge logic (Reads Test Data, Writes to Test DB/S3)
├── sps_module/         # Shared SPS logic (Ported)
├── utils/              # Modified utilities (Read/Write Bucket Separation)
├── infrastructure/     # Terraform IaC (Creates *-test resources)
├── scripts/            # Orchestration scripts (Includes monitoring)
└── Dockerfile          # Test container definition (Includes monitoring tools)
```

## Read/Write Separation Strategy

To ensure safe testing without polluting production data, the `utils/constants.py` and `utils/common.py` have been modified:

*   **Read Operations**: Default to `READ_BUCKET_NAME` ("spotlake"). This allows the test collector to read shared configurations like `sps_metadata.yaml` or region maps from the production bucket.
*   **Write Operations**: Default to `WRITE_BUCKET_NAME` ("spotlake-test"). All collected data (SPS, Price, IF) and merged results are saved here.
*   **Explicit Overrides**: Scripts like `merge_data.py` explicitly specify `bucket_name=STORAGE_CONST.WRITE_BUCKET_NAME` when reading back the raw data generated by the test collectors, ensuring they process the *test* data, not production data.

## Enhanced Monitoring

The test environment includes additional monitoring to validate performance and resource usage:

*   **Memory Monitoring**: `scripts/run_collection.sh` launches a background process that tracks RSS memory usage of the Python collectors every 10 seconds.
*   **Execution Stats**: Start time, collection duration, and merge duration are captured.
*   **Logs**: These stats are uploaded to `s3://spotlake-test/rawdata/azure/localfile/` for analysis.

## Deployment and Usage

### 1. Build and Push Image
Use the script in `scripts/` (ensure you are in the project root):

```bash
# Example (adjust paths as needed if running from root)
./collector/spot-dataset/azure/batch-test/scripts/build_and_push.sh ...
```

### 2. Deploy Infrastructure
Navigate to `infrastructure/` and use Terraform:

```bash
cd collector/spot-dataset/azure/batch-test/infrastructure
terraform init
terraform apply -var="image_uri=..." -var='subnet_ids=["..."]' ...
```

### 3. Run Locally
To test the full collection pipeline locally:

```bash
./collector/spot-dataset/azure/batch-test/scripts/run_collection.sh "2025-12-15 10:00"
```

## Important Notes

*   **Azure Auth**: The test environment currently shares the `AzureAuth` DynamoDB table for credentials (read-only). Ensure this table exists and has valid credentials.
*   **Cost**: Running this test environment creates real AWS Batch resources (Spot Instances) and makes real API calls to Azure. Monitor costs accordingly.
